This project demonstrates memory-efficient fine-tuning of Googleâ€™s Gemma-2B language model using QLoRA (Quantized Low-Rank Adaptation). The pretrained model is first quantized to 4-bit precision using NF4 to significantly reduce GPU memory usage, while LoRA adapters are trained on top of the frozen base model for task adaptation. Fine-tuning is performed on the Abirate/english_quotes dataset using supervised fine-tuning, enabling the model to generate structured and stylistically coherent quotes. This approach achieves performance comparable to full fine-tuning while requiring only a fraction of the computational resources.
