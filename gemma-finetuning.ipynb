{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.12.12","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"gpu","dataSources":[],"dockerImageVersionId":31260,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":true}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"markdown","source":"**Finetuning the gemma model**","metadata":{}},{"cell_type":"code","source":"!pip install -q -U transformers accelerate peft bitsandbytes trl datasets","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2026-02-03T17:08:26.706260Z","iopub.execute_input":"2026-02-03T17:08:26.706613Z","iopub.status.idle":"2026-02-03T17:08:30.725330Z","shell.execute_reply.started":"2026-02-03T17:08:26.706588Z","shell.execute_reply":"2026-02-03T17:08:30.724563Z"}},"outputs":[{"name":"stderr","text":"huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\nTo disable this warning, you can either:\n\t- Avoid using `tokenizers` before the fork if possible\n\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n","output_type":"stream"}],"execution_count":41},{"cell_type":"code","source":"import os\nimport torch\nimport transformers\nfrom datasets import load_dataset\nfrom trl import SFTTrainer, SFTConfig\nfrom peft import LoraConfig\nfrom transformers import (\nAutoTokenizer, \nAutoModelForCausalLM,\nBitsAndBytesConfig,\nGemmaTokenizer\n)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2026-02-03T17:08:30.726994Z","iopub.execute_input":"2026-02-03T17:08:30.727276Z","iopub.status.idle":"2026-02-03T17:08:30.731940Z","shell.execute_reply.started":"2026-02-03T17:08:30.727247Z","shell.execute_reply":"2026-02-03T17:08:30.731112Z"}},"outputs":[],"execution_count":42},{"cell_type":"code","source":"from kaggle_secrets import UserSecretsClient\nuser_secrets = UserSecretsClient()\nsecret_value_0 = user_secrets.get_secret(\"HF_TOKEN\")\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2026-02-03T17:08:30.732945Z","iopub.execute_input":"2026-02-03T17:08:30.733223Z","iopub.status.idle":"2026-02-03T17:08:30.851824Z","shell.execute_reply.started":"2026-02-03T17:08:30.733200Z","shell.execute_reply":"2026-02-03T17:08:30.850991Z"}},"outputs":[],"execution_count":43},{"cell_type":"code","source":"from kaggle_secrets import UserSecretsClient\nimport os\n\nuser_secrets = UserSecretsClient()\nhf_token = user_secrets.get_secret(\"HF_TOKEN\")\n\nos.environ[\"HF_TOKEN\"] = hf_token\n\n\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2026-02-03T17:47:45.646566Z","iopub.execute_input":"2026-02-03T17:47:45.647285Z","iopub.status.idle":"2026-02-03T17:47:45.758937Z","shell.execute_reply.started":"2026-02-03T17:47:45.647253Z","shell.execute_reply":"2026-02-03T17:47:45.758319Z"}},"outputs":[],"execution_count":69},{"cell_type":"code","source":"model_id = \"google/gemma-2b\"\nbnb_config = BitsAndBytesConfig(\n    load_in_4bit=True,\n    bnb_4bit_quant_type=\"nf4\",\n    bnb_4bit_compute_dtype=torch.bfloat16\n)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2026-02-03T17:08:30.951712Z","iopub.execute_input":"2026-02-03T17:08:30.952037Z","iopub.status.idle":"2026-02-03T17:08:30.955815Z","shell.execute_reply.started":"2026-02-03T17:08:30.952011Z","shell.execute_reply":"2026-02-03T17:08:30.955072Z"}},"outputs":[],"execution_count":45},{"cell_type":"code","source":"tokenizer = AutoTokenizer.from_pretrained(model_id,\n                                         token=os.environ[\"HF_TOKEN\"])\nmodel = AutoModelForCausalLM.from_pretrained(model_id,\n                                            quantization_config=bnb_config,\n                                            device_map={\"\":0},\n                                            token=os.environ[\"HF_TOKEN\"])\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2026-02-03T17:08:30.956729Z","iopub.execute_input":"2026-02-03T17:08:30.956973Z","iopub.status.idle":"2026-02-03T17:08:35.852114Z","shell.execute_reply.started":"2026-02-03T17:08:30.956946Z","shell.execute_reply":"2026-02-03T17:08:35.851296Z"}},"outputs":[{"output_type":"display_data","data":{"text/plain":"Loading weights:   0%|          | 0/164 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"aa71f331f3b24f049e8d7c3269a393a7"}},"metadata":{}}],"execution_count":46},{"cell_type":"code","source":"text = \"Quote: Be yourself; everyone else is already taken.\"\ndevice = \"cuda:0\"\ninputs = tokenizer(text, return_tensors=\"pt\").to(device)\n\noutputs = model.generate(**inputs, max_new_tokens=50)\nprint(tokenizer.decode(outputs[0], skip_special_tokens=True))","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2026-02-03T17:08:35.853379Z","iopub.execute_input":"2026-02-03T17:08:35.854068Z","iopub.status.idle":"2026-02-03T17:08:37.779845Z","shell.execute_reply.started":"2026-02-03T17:08:35.854043Z","shell.execute_reply":"2026-02-03T17:08:37.779194Z"}},"outputs":[{"name":"stdout","text":"Quote: Be yourself; everyone else is already taken.\n\nThe quote is attributed to Oscar Wilde, but it is not clear who actually said it.\n\nThe quote is often used as a way to encourage people to be themselves and not to conform to the expectations of others.\n\nThe quote is often used\n","output_type":"stream"}],"execution_count":47},{"cell_type":"code","source":"os.environ[\"WANDB_DISABLED\"]=\"false\"","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2026-02-03T17:08:37.780832Z","iopub.execute_input":"2026-02-03T17:08:37.781152Z","iopub.status.idle":"2026-02-03T17:08:37.784646Z","shell.execute_reply.started":"2026-02-03T17:08:37.781102Z","shell.execute_reply":"2026-02-03T17:08:37.783916Z"}},"outputs":[],"execution_count":48},{"cell_type":"code","source":"lora_config = LoraConfig(\nr=8,\ntarget_modules = [\"q_proj\", \"o_proj\", \"k_proj\", \"v_proj\",\n\"gate_proj\", \"up_proj\", \"down_proj\"],\ntask_type = \"CAUSAL_LM\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2026-02-03T17:08:37.786247Z","iopub.execute_input":"2026-02-03T17:08:37.786492Z","iopub.status.idle":"2026-02-03T17:08:37.796087Z","shell.execute_reply.started":"2026-02-03T17:08:37.786465Z","shell.execute_reply":"2026-02-03T17:08:37.795352Z"}},"outputs":[],"execution_count":49},{"cell_type":"code","source":"from datasets import load_dataset\n\ndata = load_dataset(\"Abirate/english_quotes\")\ndata = data.map(lambda samples: tokenizer(samples[\"quote\"]), batched=True)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2026-02-03T17:08:37.798567Z","iopub.execute_input":"2026-02-03T17:08:37.798882Z","iopub.status.idle":"2026-02-03T17:08:43.344085Z","shell.execute_reply.started":"2026-02-03T17:08:37.798834Z","shell.execute_reply":"2026-02-03T17:08:43.343521Z"}},"outputs":[],"execution_count":50},{"cell_type":"code","source":"data[\"train\"][\"quote\"]","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2026-02-03T17:08:43.344913Z","iopub.execute_input":"2026-02-03T17:08:43.345211Z","iopub.status.idle":"2026-02-03T17:08:43.350927Z","shell.execute_reply.started":"2026-02-03T17:08:43.345171Z","shell.execute_reply":"2026-02-03T17:08:43.350298Z"}},"outputs":[{"execution_count":51,"output_type":"execute_result","data":{"text/plain":"Column(['“Be yourself; everyone else is already taken.”', \"“I'm selfish, impatient and a little insecure. I make mistakes, I am out of control and at times hard to handle. But if you can't handle me at my worst, then you sure as hell don't deserve me at my best.”\", \"“Two things are infinite: the universe and human stupidity; and I'm not sure about the universe.”\", '“So many books, so little time.”', '“A room without books is like a body without a soul.”', ...])"},"metadata":{}}],"execution_count":51},{"cell_type":"code","source":"data[\"train\"]","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2026-02-03T17:08:43.351980Z","iopub.execute_input":"2026-02-03T17:08:43.352742Z","iopub.status.idle":"2026-02-03T17:08:43.365094Z","shell.execute_reply.started":"2026-02-03T17:08:43.352691Z","shell.execute_reply":"2026-02-03T17:08:43.364291Z"}},"outputs":[{"execution_count":52,"output_type":"execute_result","data":{"text/plain":"Dataset({\n    features: ['quote', 'author', 'tags', 'input_ids', 'attention_mask'],\n    num_rows: 2508\n})"},"metadata":{}}],"execution_count":52},{"cell_type":"code","source":"def formatting_func(example):\n    texts = []\n    for quote, author in zip(example[\"quote\"], example[\"author\"]):\n        text = f'Quote: {quote}\\nAuthor: {author}'\n        texts.append(text)\n    return texts","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2026-02-03T17:08:43.366342Z","iopub.execute_input":"2026-02-03T17:08:43.366669Z","iopub.status.idle":"2026-02-03T17:08:43.378094Z","shell.execute_reply.started":"2026-02-03T17:08:43.366631Z","shell.execute_reply":"2026-02-03T17:08:43.377456Z"}},"outputs":[],"execution_count":53},{"cell_type":"code","source":"trainer = SFTTrainer(\n    model=model,\n    train_dataset=data[\"train\"],\n    args=SFTConfig(\n        per_device_train_batch_size=1,\n        gradient_accumulation_steps=4,\n        warmup_steps=2,\n        max_steps=100,\n        learning_rate=2e-4,\n        fp16=False,\n        logging_steps=10,\n        output_dir=\"outputs\",\n        optim=\"paged_adamw_8bit\"\n    ),\n    peft_config=lora_config,\n    formatting_func=formatting_func,\n    processing_class=tokenizer\n)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2026-02-03T17:08:43.379250Z","iopub.execute_input":"2026-02-03T17:08:43.379950Z","iopub.status.idle":"2026-02-03T17:08:43.838755Z","shell.execute_reply.started":"2026-02-03T17:08:43.379916Z","shell.execute_reply":"2026-02-03T17:08:43.838120Z"}},"outputs":[{"name":"stderr","text":"WARNING:trl.trainer.sft_trainer:You passed a dataset that is already processed (contains an `input_ids` field) together with a formatting function. Therefore `formatting_func` will be ignored. Either remove the `formatting_func` or pass a dataset that is not already processed.\n","output_type":"stream"}],"execution_count":54},{"cell_type":"code","source":"trainer.train()","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2026-02-03T17:08:43.839666Z","iopub.execute_input":"2026-02-03T17:08:43.839919Z","iopub.status.idle":"2026-02-03T17:14:18.524196Z","shell.execute_reply.started":"2026-02-03T17:08:43.839897Z","shell.execute_reply":"2026-02-03T17:14:18.523420Z"}},"outputs":[{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":"\n    <div>\n      \n      <progress value='100' max='100' style='width:300px; height:20px; vertical-align: middle;'></progress>\n      [100/100 05:30, Epoch 0/1]\n    </div>\n    <table border=\"1\" class=\"dataframe\">\n  <thead>\n <tr style=\"text-align: left;\">\n      <th>Step</th>\n      <th>Training Loss</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <td>10</td>\n      <td>2.479064</td>\n    </tr>\n    <tr>\n      <td>20</td>\n      <td>1.869970</td>\n    </tr>\n    <tr>\n      <td>30</td>\n      <td>2.007760</td>\n    </tr>\n    <tr>\n      <td>40</td>\n      <td>2.229466</td>\n    </tr>\n    <tr>\n      <td>50</td>\n      <td>1.845597</td>\n    </tr>\n    <tr>\n      <td>60</td>\n      <td>2.046134</td>\n    </tr>\n    <tr>\n      <td>70</td>\n      <td>1.892348</td>\n    </tr>\n    <tr>\n      <td>80</td>\n      <td>1.800220</td>\n    </tr>\n    <tr>\n      <td>90</td>\n      <td>2.233510</td>\n    </tr>\n    <tr>\n      <td>100</td>\n      <td>2.180980</td>\n    </tr>\n  </tbody>\n</table><p>"},"metadata":{}},{"execution_count":55,"output_type":"execute_result","data":{"text/plain":"TrainOutput(global_step=100, training_loss=2.0585047149658204, metrics={'train_runtime': 334.129, 'train_samples_per_second': 1.197, 'train_steps_per_second': 0.299, 'total_flos': 189744345784320.0, 'train_loss': 2.0585047149658204})"},"metadata":{}}],"execution_count":55},{"cell_type":"code","source":"new_model = \"gemma_quotes_finetuned\"\ntrainer.model.save_pretrained(new_model)\ntokenizer.save_pretrained(new_model)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2026-02-03T17:14:18.525336Z","iopub.execute_input":"2026-02-03T17:14:18.525629Z","iopub.status.idle":"2026-02-03T17:14:19.208871Z","shell.execute_reply.started":"2026-02-03T17:14:18.525605Z","shell.execute_reply":"2026-02-03T17:14:19.208116Z"}},"outputs":[{"execution_count":56,"output_type":"execute_result","data":{"text/plain":"('gemma_quotes_finetuned/tokenizer_config.json',\n 'gemma_quotes_finetuned/tokenizer.json')"},"metadata":{}}],"execution_count":56},{"cell_type":"code","source":"from peft import AutoPeftModelForCausalLM\n\n# LOAD the model from the path\nfinetuned_model = AutoPeftModelForCausalLM.from_pretrained(\n    new_model,  # Load from this path\n    device_map={\"\": 0},\n    torch_dtype=torch.bfloat16\n)\n\n# merges LoRA weights into the base model\nfinetuned_model = finetuned_model.merge_and_unload()","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2026-02-03T17:14:19.209916Z","iopub.execute_input":"2026-02-03T17:14:19.210188Z","iopub.status.idle":"2026-02-03T17:14:26.067089Z","shell.execute_reply.started":"2026-02-03T17:14:19.210164Z","shell.execute_reply":"2026-02-03T17:14:26.066538Z"}},"outputs":[{"output_type":"display_data","data":{"text/plain":"Loading weights:   0%|          | 0/164 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"dd5fa8f13d7d4d23849ca20d67745b48"}},"metadata":{}}],"execution_count":57},{"cell_type":"code","source":"# loaded model\ntext = \"Quote: Logic will get you from A to Z; imagination will get you everywhere.\\nAuthor:\"\ninputs = tokenizer(text, return_tensors=\"pt\").to(finetuned_model.device)\n\noutputs = finetuned_model.generate(\n    **inputs,\n    max_new_tokens=20,  \n    do_sample=True,\n    temperature=0.7,\n    top_p=0.9,\n    repetition_penalty=1.2,\n    no_repeat_ngram_size=3,\n    pad_token_id=tokenizer.eos_token_id\n)\n\nprint(tokenizer.decode(outputs[0], skip_special_tokens=True))","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2026-02-03T17:14:26.067981Z","iopub.execute_input":"2026-02-03T17:14:26.068291Z","iopub.status.idle":"2026-02-03T17:14:26.819541Z","shell.execute_reply.started":"2026-02-03T17:14:26.068268Z","shell.execute_reply":"2026-02-03T17:14:26.818746Z"}},"outputs":[{"name":"stdout","text":"Quote: Logic will get you from A to Z; imagination will get you everywhere.\nAuthor: Albert Einstein\n\nI don't know if it was a typo, but I thought the quote said\n","output_type":"stream"}],"execution_count":58},{"cell_type":"code","source":"# loaded model\ntext = \"Quote: Of course it is happening inside your head, Harry.\"\ninputs = tokenizer(text, return_tensors=\"pt\").to(finetuned_model.device)\n\noutputs = finetuned_model.generate(\n    **inputs,\n    max_new_tokens=20,  \n    do_sample=True,\n    temperature=0.7,\n    top_p=0.9,\n    repetition_penalty=1.2,\n    no_repeat_ngram_size=3,\n    pad_token_id=tokenizer.eos_token_id\n)\n\nprint(tokenizer.decode(outputs[0], skip_special_tokens=True))","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2026-02-03T17:16:47.812219Z","iopub.execute_input":"2026-02-03T17:16:47.813003Z","iopub.status.idle":"2026-02-03T17:16:48.501903Z","shell.execute_reply.started":"2026-02-03T17:16:47.812970Z","shell.execute_reply":"2026-02-03T17:16:48.501150Z"}},"outputs":[{"name":"stdout","text":"Quote: Of course it is happening inside your head, Harry. But why on earth should that mean that it's not real?\n\nHarry Potter and the Sorcerer\n","output_type":"stream"}],"execution_count":68}]}